\section{Introduction}

Many physical and astrophysical systems are subject to non-linear dynamics
and rely on N-body simulations to describe the evolution of bodies. 
One of the main field of application is the modelling of large scale structures, 
which are driven by the sole force of gravity. Recent observations of the 
cosmic microwave background \citep{2009ApJS..180..330K,2011ApJS..192...18K}, of galaxy clustering 
\citep{2000AJ....120.1579Y, 2003astro.ph..6581C, 2009arXiv0902.4680S, 
2010MNRAS.401.1429D} of weak gravitational lensing \citep{2012AAS...21913001H, 2009ApJ...703.2232S}
and of supernovae redshift-distance relations all point towards a standard 
model of cosmology, in which dark energy and collision-less dark matter occupy 
more than 95 per cent of the total energy density of the universe. In such a 
paradigm, pure N-body code are perfectly suited to describe the dynamics, as 
long as baryonic physics is not very important, or at least we understand how 
the baryonic fluid feeds back on the dark matter structure. The next generation 
of measurements aim at constraining the cosmological parameters at the per cent 
level, and the theoretical understanding of the non-linear dynamics that govern 
structure formation heavily relies on numerical simulations. 

For instance, a measurement of the baryonic acoustic oscillation (BAO) dilation 
scale can provide tight constraints on the dark energy equation of scale 
\citep{Eisenstein:2005su,2006PhRvD..74l3507T, 2007MNRAS.381.1053P,2009arXiv0902.4680S}. 
The most optimal estimates of the uncertainty requires the 
knowledge of the matter power spectrum covariance matrix, which is only accurate 
when measured from a large sample of N-body simulations \citep{2005MNRAS.360L..82R, 
2009ApJ...700..479T, 2011ApJ...726....7T}. For the same reasons, the most accurate 
estimates of weak gravitational lensing signal is obtained by propagating photons 
in past light cones that are extracted from simulated density fields 
\citep{2003ApJ...592..699V, 2009ApJ...701..945S, 2009A&A...499...31H}.
Another area where large-scale N-body simulations have in recent years been 
instrumental are in simulations of early cosmic structures and reionization 
\citep[e.g.][]{2006MNRAS.369.1625I,2007ApJ...654...12Z,2007ApJ...671....1T,
2011arXiv1107.4772I}. The reionization process is primarily driven by low-mass 
galaxies, which for both observational and theoretical reasons, need to be resolved 
in fairly large volumes, which demands simulations with a very large dynamic range.   
%{\bf (Ilian, could you say something about reionization here? Just general references as to why one needs N-body codes to perform 
%accurate predictions/forecasts, etc.)}

The basic problem that is addressed with N-body codes is a time evolution of an ensemble of $N$ particles
that is subject to gravitational attraction. The brute force calculation requires $O(N^{2})$ operation, a cost that 
exceeds the memory and speed of current machines for large problems.
Solving the problem  on a mesh \citep{1981csup.book.....H} reduces to $O(N\mbox{log}N)$ the number of operations,
as it is possible to solve for the particle-mesh (PM) interaction with fast Fourier transforms techniques, 
typically using high performance libraries such as {\small FFTW} \citep{FFTW3}.


With the advent of large computing facilities, parallel coding has now become 
common practice, and N-body codes have evolved both in performance and complexity. 
Many codes have opted for a tree algorithm, including {\small GADGET} \citep{2001NewA....6...79S, 2005MNRAS.364.1105S}, PMT \citep{1995ApJS...98..355X}, GOTPM \citep{2004NewA....9..111D}, and Hydra \citep{1995ApJ...452..797C}, in which the local resolution increases with the density of the matter field. 
These have the advantage to balance the load across the computing units, which enable the calculation of high density regions. 
The drawback is a significant loss in speed, which can be only partly recovered by turning off the tree algorithm. 
The same reasoning applies to the mesh-refine code of  \cite{1991ApJ...368L..23C} and the PM code by \cite{1995astro.ph..3042F}.
 these are not designed to perform large scale PM calculations. 

{\bf ( PM code by japanese? Others codes I should mention?)}


{\small PMFAST} (\cite{2005NewA...10..393M}, MPT hereafter) is one of the first code that is designed such as to optimize the PM algorithm,
both in terms of speed and memory usage. It uses a two-level mesh algorithm based on the gravity solver of \cite{2003AAS...203.9703T},
The long range gravitational force is computed on a  grid four times coarser, such as to minimize the {\small MPI} communication time
and to fit in system's memory. The short range is computed locally on a finer mesh, and only the local sub-volume needs 
to be stored at a given time, allowing for {\small OPENMP} parallel computation.
This this setup enable the code to evolve large cosmological systems both rapidly and accurately, on relatively modest clusters.
One of the main advantage of {\small PMFAST} over other PM codes is that the number of large arrays is minimized,
and the global {\small MPI} communications are cut down to the minimum: for passing particle at the beginning of each time step,
and  for computing the long range FFTs.
As described in MPT, access to particles is accelerated with the use of linked lists, deletion of `ghost' particles
in buffer zones is done at the same time as particles are passed to adjacent nodes,
and the global FFTs are performed with a slab decomposition of the volumes via a special file transfer interface, 
designed specifically to preserve a high processor load.

Since its first published version, {\small PMFAST} has evolved in many aspects. 
The first major improvement was to transform the volume decomposition in multi-node configurations 
from slabs to cubes. One of the problem with slabs is that they do not scale well to large runs: 
as the number of cells per dimension increases, the thickness of each slab shrinks rapidly,
until it reaches the hard limit of a single cell layer.  With this enhancement, the code name was changed to {\small CUBEPM},
which soon after incorporated particle-particle (pp) interactions at the sub-grid level. 
The code was finally renamed {\small CUBEP3M}, and now includes a significant number of new features: the pp force
can be extended to an arbitrary range, the size of the redshift jump can be constrained for improved accuracy during the first time steps,
a runtime halo finder has been implemented, the expansion has also been generalized to include a redshift dependent equation of state of dark energy, there is a system of unique particle identification which can be switched on or off, and the initial condition generator has been generalized as to include non-Gaussian features {\bf (anything else since PMFAST?)}.

This paper aims at presenting and quantifying these new features that make {\small CUBEP3M} one of the most competitive public N-body code.
It has already been involved in a number of scientific applications over the last few years,
spanning the field of weak lensing \citep{Vafaei10, 2008MNRAS.388.1819L,  2009arXiv0905.0501D, 2010PhRvD..81l3015L, 2010arXiv1012.0444Y, 2012arXiv1202.2332H},  BAO
 (\cite{2010arXiv1008.3506Z,  2011arXiv1106.5548N, 2011arXiv1109.5746H}; Harnois-D\'{eraps} et al 2012 (in preparation)), 
formation of early cosmic structures \citep{2008arXiv0806.2887I,2010arXiv1005.2502I},
observations of dark stars \citep{2010MNRAS.407L..74Z,2012MNRAS.tmp.2794I} and
reionization \citep{2011arXiv1107.4772I,Fernandez:2011ab,2011MNRAS.413.1353F,2012MNRAS.422..926M,Datta:2011hv,2012arXiv1203.0517F},  
and it seems relevant for the community to have access to a paper that describes the methodology, the accuracy and the performance of this public code. 

Since {\small CUBEP3M} is not a new code, the accuracy and systematic tests were performed by different groups, on different machines,
and with different configurations. It is not an ideal situation in which to quantify the performance, and each test must be viewed as a
separate measurement that quantifies a specific aspect of the code. 
We have tried to keep to a minimum the number of such different runs, and although the detailed numbers might vary across different runs, 
the general trends are common to all of them.


The paper is structured as follow: XXXX 



