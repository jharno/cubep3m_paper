\section{Introduction}

Many physical and astrophysical systems are subject to non-linear dynamics
and rely on N-body simulations to describe the evolution of bodies. 
One of the main field of application is the modelling of large scale structures, 
which are driven by the sole force of gravity. Recent observations of the 
cosmic microwave background \citep{2009ApJS..180..330K,2011ApJS..192...18K}, of galaxy clustering 
\citep{2000AJ....120.1579Y, 2003astro.ph..6581C, 2009arXiv0902.4680S, 
2010MNRAS.401.1429D} of weak gravitational lensing \citep{2012AAS...21913001H, 2009ApJ...703.2232S}
and of supernovae redshift-distance relations all point towards a standard 
model of cosmology, in which dark energy and collision-less dark matter occupy 
more than 95 per cent of the total energy density of the universe. In such a 
paradigm, pure N-body code are perfectly suited to describe the dynamics, as 
long as baryonic physics is not very important, or at least we understand how 
the baryonic fluid feeds back on the dark matter structure. The next generation 
of measurements aim at constraining the cosmological parameters at the per cent 
level, and the theoretical understanding of the non-linear dynamics that govern 
structure formation heavily relies on numerical simulations. 

For instance, a measurement of the baryonic acoustic oscillation (BAO) dilation 
scale can provide tight constraints on the dark energy equation of scale 
\citep{Eisenstein:2005su,2006PhRvD..74l3507T, 2007MNRAS.381.1053P,2009arXiv0902.4680S}. 
The most optimal estimates of the uncertainty requires the 
knowledge of the matter power spectrum covariance matrix, which is only accurate 
when measured from a large sample of N-body simulations \citep{2005MNRAS.360L..82R, 
2009ApJ...700..479T, 2011ApJ...726....7T}. For the same reasons, the most accurate 
estimates of weak gravitational lensing signal is obtained by propagating photons 
in past light cones that are extracted from simulated density fields 
\citep{2003ApJ...592..699V, 2009ApJ...701..945S, 2009A&A...499...31H}.
Another area where large-scale N-body simulations have in recent years been 
instrumental are in simulations of early cosmic structures and reionization 
\citep[e.g.][]{2006MNRAS.369.1625I,2007ApJ...654...12Z,2007ApJ...671....1T,
2011arXiv1107.4772I}. The reionization process is primarily driven by low-mass 
galaxies, which for both observational and theoretical reasons, need to be resolved 
in fairly large volumes, which demands simulations with a very large dynamic range.   
%{\bf (Ilian, could you say something about reionization here? Just general references as to why one needs N-body codes to perform 
%accurate predictions/forecasts, etc.)}

The basic problem that is addressed with N-body codes is the time evolution of an ensemble of $N$ particles
that is subject to gravitational attraction. The brute force calculation requires $O(N^{2})$ operations, a cost that 
exceeds the memory and speed of current machines for large problems.
Solving the problem  on a mesh \citep{1981csup.book.....H} reduces to $O(N\mbox{log}N)$ the number of operations,
as it is possible to solve for the particle-mesh (PM) interaction with fast Fourier transforms techniques with high performance libraries such as {\small FFTW} \citep{FFTW3}.


With the advent of large computing facilities, parallel computations have now become 
common practice, and N-body codes have evolved both in performance and complexity. 
Many have opted for  `tree' algorithms, including {\small GADGET} \citep{2001NewA....6...79S, 2005MNRAS.364.1105S}, {\small PMT} \citep{1995ApJS...98..355X}, {\small GOTPM} \citep{2004NewA....9..111D}, and Hydra \citep{1995ApJ...452..797C}, in which the local resolution increases with the density of the matter field. 
These often have the advantage to balance the work load across the computing units, which enable fast calculations even in high density regions. 
The drawback is a significant loss in speed, which can be only partly recovered by turning off the tree algorithm. 
The same reasoning applies to mesh-refined codes  \citep{1991ApJ...368L..23C}, 
which in the end are not designed to perform fast PM calculations on large scales. 

%{\bf ( PM code by japanese? Others codes I should mention?)}


{\small PMFAST} (\cite{2005NewA...10..393M}, MPT hereafter) is one of the first code designed specifically to optimize the PM algorithm,
both in terms of speed and memory usage. It uses a two-level mesh algorithm based on the gravity solver of \cite{2003AAS...203.9703T},
The long range gravitational force is computed on a  grid four times coarser, such as to minimize the {\small MPI} communication time
and to fit in system's memory. The short range is computed locally on a finer mesh, and only the local sub-volume needs 
to be stored at a given time, allowing for {\small OPENMP} parallel computation.
This this setup enable the code to evolve large cosmological systems both rapidly and accurately, on relatively modest clusters.
One of the main advantage of {\small PMFAST} over other PM codes is that the number of large arrays is minimized,
and the global {\small MPI} communications are cut down to the minimum: for passing particle at the beginning of each time step,
and  for computing the long range FFTs.
As described in MPT, access to particles is accelerated with the use of linked lists, deletion of `ghost' particles
in buffer zones is done at the same time as particles are passed to adjacent nodes,
and the global FFTs are performed with a slab decomposition of the volumes via a special file transfer interface, 
designed specifically to preserve a high processor load.

Since its first published version, {\small PMFAST} has evolved in many aspects. 
The first major improvement was to transform the volume decomposition in multi-node configurations 
from slabs to cubes. One of the problem with slabs is that they do not scale well to large runs: 
as the number of cells per dimension increases, the thickness of each slab shrinks rapidly,
until it reaches the hard limit of a single cell layer.  With this enhancement, the code name was changed to {\small CUBEPM},
which soon after incorporated particle-particle (pp) interactions at the sub-grid level. 
The code was finally renamed {\small CUBEP3M}, and now includes a significant number of new features: the pp force
can be extended to an arbitrary range, the size of the redshift jump can be constrained for improved accuracy during the first time steps,
a runtime halo finder has been implemented, the expansion has also been generalized to include a redshift dependent equation of state of dark energy, there is a system of unique particle identification which can be switched on or off, and the initial condition generator has been generalized as to include non-Gaussian features.
 {\small PMFAST} was also equipped with a multi time stepping option that has not been tested on {\small CUBEP3M} yet, but which is, in principle at least, still available. 
 It also contains support for gas cosmological evolution through a TVD MHD module,
 and a coupling interface with the radiative transfer code C2-Ray \citep{2006NewA...11..374M}. 

This paper aims at presenting and quantifying these new features that make {\small CUBEP3M} one of the most competitive public N-body code.
It has already been involved in a number of scientific applications over the last few years,
spanning the field of weak lensing \citep{Vafaei10, 2008MNRAS.388.1819L,  2009arXiv0905.0501D, 2010PhRvD..81l3015L, 2010arXiv1012.0444Y, 2012arXiv1202.2332H},  BAO
 \citep{2010arXiv1008.3506Z,  2011arXiv1106.5548N, 2011arXiv1109.5746H, 2012arXiv1205.4989H}, 
formation of early cosmic structures \citep{2008arXiv0806.2887I,2010arXiv1005.2502I},
observations of dark stars \citep{2010MNRAS.407L..74Z,2012MNRAS.tmp.2794I} and
reionization \citep{2011arXiv1107.4772I,Fernandez:2011ab,2011MNRAS.413.1353F,2012MNRAS.422..926M,Datta:2011hv,2012arXiv1203.0517F},  
and it is thus important for the community to have access to a paper that describes the methodology, the accuracy and the performance of this public code. 

%\subsection{Simulation suites}
%\label{subsec:suites}

Since {\small CUBEP3M} is not a new code, the accuracy and systematic tests were performed by different groups, on different machines,
and with different configurations. It is not an ideal situation in which to quantify the performance, and each test must be viewed as a
separate measurement that quantifies a specific aspect of the code. 
We have tried to keep to a minimum the number of such different runs, and although the detailed numbers might vary across different runs, 
the general trends are common to all of them.

Tests on constraints of the redshift step size were performed 
on the Sunnyvale beowulf cluster at the Canadian Institute for Theoretical Astrophysics (CITA).
Each node contains 2 Quad Core Intel(R) Xeon(R) E5310 1.60GHz processors, 4GB of RAM,  a 40 GB disk and 2 gigE network interfaces. 
We generate a set of Gaussian initial conditions at a starting redshift of $z = 100$, 
with a box size equal to $200 h^{-1}\mbox{Mpc}$, we placed
$128^{3}$  particles on a grid with $2^{3}$ more cells, and let them evolve until $z=0$.
Tests on improvements of the force calculations were performed on the same machine, but with 8 times more particles and node, 
and with a box size of $500 h^{-1}\mbox{Mpc}$.
Hereafter we refer to these simulation modes as the CITA128 and CITA256 configurations respectively.

For tests of the accuracy over a large dynamical range of the simulation, of the non-Gaussian initial conditions generator and of the run time halo finder algorithm, 
we used a third simulation configuration that were run at the Texas Advanced Computing Centre (TACC) on Ranger, a SunBlade 
x6420 system with AMD x86\_64 Opteron Quad Core 2.3 GHz `Barcelona' processors and Infiniband networking.
These RANGER4000 simulations evolved $4000^{3}$ particles 
from $z=100$ to $z=0$ with a box side of $3.2 h^{-1}\mbox{Gpc}$
on 4000 cores.
 
 Much larger runs were produced at various high performance computing centers
 around the world; their description and a comparison of their performance are also described in a later section of this paper. 
 

The paper is structured as follow: section \ref{sec:structure} reviews the structure and flow of the main code;
section \ref{sec:Poisson} describes how Poisson equation is solved on the two-mesh system;
we then present in section \ref{sec:scaling} the scaling of the code to very large problems;
sections \ref{sec:accuracy} and \ref{sec:systematics} discuss the accuracy and systematic effects of the code respectively.
 We then describe in section \ref{sec:halo} the run-time halo finder, in section \ref{sec:extensions} various extensions to 
 to the default configuration, and conclude afterwards.




