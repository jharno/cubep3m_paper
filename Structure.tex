
\section{Review of the Code Structure}
\label{sec:structure}


An optimal large scale N-body code must address many challenges: minimize the memory footprint to allow larger dynamical range,
minimize the passing of information across computing nodes, reduce and accelerate the memory accesses to the large scale arrays, 
make efficient use of high performance libraries to speed up standard calculations like Fourier transforms, just to name a few.
In the realm of parallel programming, high efficiency  can be assessed when a high load is balanced across all processors
most of the time. In this section, we present the general strategies adopted to address these challenges\footnote{ 
Many originate directly from MPT and were preserved in {\small CUBEP3M};
those will be briefly mentioned, and we shall refer the reader to the original {\small PMFAST} paper for greater details.}.
We start with a walkthrough the code flow, and briefly discuss some specific sections that depart from standard N-body codes,
while referring the reader to future sections for detailed discussions on selected topics.


As mentioned in the Introduction section, {\small CUBEP3M} is a {\small FORTRAN90} 
N-body code that solves Poisson's equation on a two-level mesh, 
with sub-cell accuracy thanks to particle-particle interactions. 
The code has extensions that departs from this basic scheme, and
we shall come back to these later, but for the moment, we adopt the 
standard configuration. 
The long range component of the gravity force is solved on the coarse grid, 
and is global in the sense that the calculations require knowledge about the full simulated volume.
The short range force and the particle-particle interactions are computed in parallel on local volumes\footnote{To make this possible, the fine grid arrays are constructed such as to support parallel reading and writing. In practice, this is done by adding an additional dimension to the relevant arrays, such that each {\small CPU} accesses a unique memory location.} or {\it tiles}. The force matching of the two meshes is performed by introducing a cutoff in the two force kernel. The value of $r_{c}=16$ fine cells was chosen such as to balance the communication 
overhead between processes and the accuracy of the match between the two meshes. 

The computation of the short range force requires each tile to store the fine grid density in a buffer surface around the physical volume it is assigned. The thickness of this surface must be larger than the cutoff length (of 16 cells), and we find that a 24 cells deep buffer
is a good compromise between memory usage and accuracy.
When it comes to finding haloes at run time can this buffer create a problem, because a large halo located close to the boundary
can have a radius larger than the buffer zone, in which case it would be truncated, thus falling in the wrong mass bin, producing an incorrect center of mass, etc. 
It could then be desirable to increase the buffer zone around each tile, at the cost of a loss of memory dedicated to the actual physical volume.


%\subsection{Memory foot-print and communication strategy}
%\label{subsec:memory}

Because the coarse grid arrays require $4^3$ times less memory per node, 
the bulk of the foot-print is concentrated in a handful of fine grid arrays.
In addition, some of these are required for intermediate steps of the calculations only, 
hence it is possible to hide some of the coarse grid arrays\footnote{ This memory recycling is done with `equivalence' statements in {\small F90}}.   
We present here the largest arrays used by the code:
\begin{enumerate}
\item{{\tt xv} stores the position and velocity of each particle} 
\item{{\tt ll} stores the linked-list that accelerate the access to particles in each coarse grid cell}
\item{{\tt rho\_f} and {\tt cmplx\_rho\_f} store 
the local fine grid density  in real and Fourier space respectively}
\item{{\tt force\_f} stores the force of gravity (short range only) along the three Cartesian directions}
\item{{\tt kern\_f} stores the fine grid force kernel in the three directions}
\item{{\tt PID} stores the unique particle identification tags.}
\end{enumerate}
The particle ID is a feature that can be switched off by removing a compilation flag, 
and allows to optimize the code for higher resolution configurations.
%\subsection{Code overview}
%\label{subsec:overview}

The code flow is presented in Fig. \ref{fig:structure} and \ref{fig:particle_mesh}.
Before entering the main loop, the code starts with an initialization stage, 
in which many declared variables are assigned default values,
the redshift checkpoints are read, the {\small FFTW} plans are created, and the {\small MPI} communicators are defined.
The phase-space array  is obtained from the output of the initial conditions generator,
and the force kernels on both grids are constructed from the specific geometry of the simulation.
For clarity, all these operations are collected under the subroutine call {\tt initialize} in Fig. \ref{fig:structure}, 
although they are actually distinct calls in the code.

Each iteration of the main loop starts with the {\tt timestep} subroutine, 
which proceeds to a determination of the redshift jump by comparing the step size constraints from each
force components and from the scale factor.
The cosmic expansion is found by Taylor expanding Friedmann's equation up to the third order in the scale factor,
and can accommodate constant or running equation of state of dark energy.
The force of gravity is then solved  in the {\tt particle\_mesh} subroutine,
which first updates the positions and velocities of the dark matter particles, exchange with neighbouring nodes those that have exited to volume,
creates a new linked list, then solve Poisson's equation.  This subroutine is conceptually identical to that of {\small PMFAST}, 
with the exception  that {\small CUBEP3M} decomposes the volume into cubes (as opposed to slabs). 
The loop over tiles and the particle exchange are thus performed in three dimensions.
{\bf (Should I mention leapfrog here or anywhere else?)}
When the short range and pp forces have been calculated on all tiles, the code exits the parallel {\small OPENMP} loop
and proceeds to the long range. This section of the code is also parallelized in many occasions, but, unfortunately, the current {\small MPI-FFTW}
do not allow multi-threading. There is thus an inevitable loss of efficiency during each global Fourier transforms, during which
only the single core {\small MPI} process is active\footnote{Other libraries such as {\small P3DFFT} ({\tt http://code.google.com/p/p3dfft/}) currently permit
 an extra level of parallelization, and it is our plan to migrate to one of these in the near future.}.

\begin{figure}
\begin{verbatim}
program cubep3m
   call initialize
   do
       call timestep
       call particle_mesh
       if(checkpoint_step) then
          call checkpoint
       elseif(last_step)
          exit
       endif
   enddo
   call finalize
end program cubep3m
\end{verbatim}
\caption{Overall structure of the code.}
\label{fig:structure}
\end{figure}

\begin{figure}
\begin{verbatim}
subroutine particle_mesh
   call update_position
   call link_list
   call particle_pass
   !$omp parallel do
   do tile = 1, tiles_node
      call rho_f_ngp
      call cmplx_rho_f
      call kernel_multiply_f
      call force_f
      call update_velocity_f
      if(pp = .true.) then       
         call link_list_pp
         call force_pp
         call update_velocity_pp
         if(extended_pp = .true.) then
            call link_list_pp_extended
            call force_pp_extended
            call update_velocity_pp_extended       
         endif
      endif
   end do
   !$omp end parallel do
   call rho_c_ngp
   call cmplx_rho_c
   call kernel_multiply_c
   call force_c
   call update_velocity_c      
   delete_buffers
end subroutine particle_mesh
\end{verbatim}
\caption{Overall structure of the two-level mesh algorithm. We have included the section that concerns the extended pp force calculation to illustrate that it follows the same basic logic. We mention here that the three linked list arrays are distinct entities, and their structure differ slightly.  }
\label{fig:particle_mesh}
\end{figure}


If the current redshift corresponds to one of the checkpoints, the code advances all particles to their final location
and writes them to file. Similarly, the code can compute two-dimensional projections of the density field, halo catalogues (see section \ref{sec:halo} for details), and can compute the power spectrum on the coarse grid at run time. 
The code exits the loop when it has reached the final redshift, it then wraps up the {\small FFTW} plans 
and clears the {\small MPI} communicators. We have collected those operations under the subroutine {\tt finalize} for concision.

Other considerations that need to be considered is that any decomposition geometry somehow 
limits permissible grid sizes, and that volume evenly decomposed into cubic sub-sections 
--  such as {\small CUBEP3M} -- requires the number of MPI processes to be a perfect cube.
Also,  since the decomposition is volumetric, as opposed to density dependent -- 
it suffers from load imbalance when probing deep into the non-linear regime.
